{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Park Mate Predict Research notebook \n",
    "### by: Trevor Loren (tloren@deakin.edu.au)\n",
    "This notebook uses RNN to predict parking density using historical parking sensor data set from historical parking data and builds and deploys the model to IBM Watson ML \n",
    "Live data: https://data.melbourne.vic.gov.au/Transport/On-street-Parking-Bay-Sensors/vh2v-4nfs\n",
    "Historical data: https://data.melbourne.vic.gov.au/Transport/On-street-Car-Parking-Sensor-Data-2020-Jan-May-/4n3a-s6rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sensor_df = pd.read_csv(\"sensorApr15-29.csv\", usecols=[\"ArrivalTime\", \"BayId\", 'DurationMinutes'])\n",
    "bay_df = pd.read_csv(\"On-street_Parking_Bay_Sensors.csv\",  usecols=[\"bay_id\", \"lat\", 'lon'])\n",
    "\n",
    "sensor_df['ArrivalTime'] = pd.to_datetime(sensor_df['ArrivalTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating bins based on lat and lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.005\n",
    "to_bin = lambda x: np.floor(x / step) * step\n",
    "bay_df['latbin'] = bay_df.lat.map(to_bin)\n",
    "bay_df['lonbin'] = bay_df.lon.map(to_bin)\n",
    "\n",
    "for i in bay_df.index:\n",
    "    bay_df.at[i, 'location'] = str(round(bay_df.at[i,'latbin'], 4))+\",\"+str(round(bay_df.at[i, 'lonbin'], 4))           \n",
    "bay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging data to remove untracked bays in live data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = pd.merge(sensor_df, bay_df[['location','bay_id']], left_on='BayId', right_on='bay_id')\n",
    "sensor_df.drop(columns=['bay_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding hourly time samples for the duration of the parking from start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def time_sampler_2(df):\n",
    "    samples = []\n",
    "    for i in df.index:\n",
    "        duration = df.at[i, 'DurationMinutes']\n",
    "        if(duration>0):\n",
    "            hours = int(duration/60)\n",
    "            for hour_delta in range(1, hours):\n",
    "                time = df.at[i,'ArrivalTime'] + timedelta(hours = hour_delta)               \n",
    "                samples.append([time, df.at[i, 'location'], df.at[i, 'BayId'], df.at[i, 'DurationMinutes']])\n",
    "    return pd.DataFrame(np.array(samples), columns=['ArrivalTime', 'location', 'BayId', 'DurationMinutes'])\n",
    "\n",
    "df = sensor_df.append(time_sampler_2(sensor_df))\n",
    "print(sensor_df.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting the data to move location to the column header with counts of occupied bays, followeed by temporal windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df1['Hour'] = df1['ArrivalTime'].dt.round('H')\n",
    "df1.head()\n",
    "df1 = df1.drop_duplicates(subset=['Hour', 'BayId'], keep='first')\n",
    "pivoted_df1 = df1.pivot_table(index='Hour', columns='location', values='BayId', aggfunc='count')\n",
    "pivoted_df1 = pivoted_df1.fillna(0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "pivoted_df1.iloc[:, :] = scaler.fit_transform(pivoted_df1.iloc[:, :].values)\n",
    "\n",
    "\n",
    "batched_data_x = []\n",
    "batched_data_y = []\n",
    "window_size = 3 #2years for yearly recurring events\n",
    "for i in range(window_size, len(pivoted_df1)):\n",
    "    batched_data_x.append(pivoted_df1.iloc[i-window_size:i].values)\n",
    "    batched_data_y.append(pivoted_df1.iloc[i].values)\n",
    "batched_data_x, batched_data_y = np.array(batched_data_x), np.array(batched_data_y)\n",
    "print(batched_data_x.shape)\n",
    "print(batched_data_y.shape)\n",
    "batched_data_x\n",
    "\n",
    "#train test split\n",
    "test_size = 20\n",
    "x_train = batched_data_x[:-test_size,:,:]\n",
    "y_train = batched_data_y[:-test_size,:]\n",
    "x_test = batched_data_x[batched_data_x.shape[0]-test_size:,:,:]\n",
    "y_test = batched_data_y[batched_data_y.shape[0]-test_size:,:]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating fitting and saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "print(\"Window size = \", x_train.shape[1])\n",
    "regressor.add(SimpleRNN(units=50, input_shape=(x_train.shape[1], x_train.shape[2]), name='simple_RNN'))\n",
    "regressor.add(Dropout(0.5, name='drp1'))\n",
    "regressor.add(Dense(units=124*8, name='fc1'))\n",
    "regressor.add(Dense(units=47, name='output'))\n",
    "\n",
    "adamopt = Adam(lr=0.0001)\n",
    "regressor.compile(optimizer = adamopt, loss='mse')\n",
    "\n",
    "model_history = regressor.fit(x_train, y_train, epochs=150, batch_size=30, validation_data=(x_test, y_test))\n",
    "\n",
    "regressor.save(\"model.h5\")\n",
    "!tar -zcvf model.tgz model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and evaluating our prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_history.history['loss'], label='train')\n",
    "plt.plot(model_history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = regressor.predict(x_test)\n",
    "err = np.mean(np.abs(y_test - test_pred))\n",
    "print('test MAE error for standard averaging:', err)\n",
    "print(test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,6))\n",
    "plt.plot(y_test[0],color='g',label='test_true')\n",
    "plt.plot(test_pred[0],color='r',label='test_pred')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close')\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and deploying the model to Watson ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "wml_credentials = {\n",
    "    #Your wml credentials from watson ml\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: \"parkmate1\",\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.15\",\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{'name':'keras', 'version': '2.2.5'}]\n",
    "}\n",
    "model_details = client.repository.store_model( model=\"model.tgz\", meta_props=metadata )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_details[\"metadata\"][\"guid\"]\n",
    "model_deployment_details = client.deployments.create( artifact_uid=model_id, name=\"ParkMateForecast\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
